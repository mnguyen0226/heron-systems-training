{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks Implementation\n",
    "Why is this important?\n",
    "- Attention mechanism in transformer has encoder and decoder \n",
    "\n",
    "Resources:\n",
    "- https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ RNN About:\n",
    "-  A neural network that is specialized for processing sequence of data\n",
    "- For NLP, you want to predict the next word in a sentence, it is important to know the words before it.\n",
    "- Recurrent = they perform the same task for every element of a sequence, with the output being depended on the previous computation. We can think that the RNN has memory which captures the info about what has been calculated so far\n",
    "- The gradient at each out put depends not only on the calculations of the current time step but also in the previous time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ Implementation:\n",
    "- Build a text generation model with RNN. \n",
    "- Train model to predict the probability of character given the preceding charaters\n",
    "- Steps:\n",
    "    - 1. Initialize weight matrices U,V,W from random distribution and bias b,c with zeros\n",
    "    - 2. Forward propagation to compute prediction\n",
    "    - 3. Compute the loss\n",
    "    - 4. Back-propagation to compute gradients\n",
    "    - 5. Update weights based on gradients\n",
    "    - 6. Repeat 2-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialization\n",
    "\n",
    "### Step 2: Forward pass\n",
    "We have a set of equation\n",
    "- a(t) = b + W*h(t-1) + U*x(t)\n",
    "- h(t) = tanh(a(t))\n",
    "- o(t) = c+ V*h(t)\n",
    "- y(t) = softmax(o(t))\n",
    "\n",
    "### Step 3: Compute Softmax and Numerical Stability\n",
    "- Softmax function take N-dim vector of real number and transfer it into a vector or real numberin range [0,1] and add up to 1\n",
    "\n",
    "### Step 4: Compute Loss\n",
    "\n",
    "### Step 5: Backward Pass\n",
    "\n",
    "### Step 6: Update Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vocab size can be the number of unique chars from a char based model or number of unique words from a word based model \n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, lr):\n",
    "        # hyper parameters:\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.lr = lr\n",
    "        \n",
    "        # model parameter - random initialization:\n",
    "        # it is recommend that the weight initialization is randomly from [-1/sqrt(n), 1/sqrt(n)] with n = the number of incoming connection from the previous layer\n",
    "        # function (low, high, size = the output shape)\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "    \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = zero_init(self.vocab_size, 1)\n",
    "            xs[t][input[t]] = 1 # one hot encoding\n",
    "            hs[t] = np.tanh(np.dot(self.U, xs[t]) + np.dot(self.W, hs[t-1]) + self.b)\n",
    "            os[t] = np.dot(self.V, hs[t]) + self.c # unnormalized log probs for the next char\n",
    "            ycap[t] = self.softax(os[t]) # probs for next char\n",
    "            \n",
    "        return xs, hs, ycap\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x - np.max(x))\n",
    "        return p/np.sum(p)\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        # Calculate cross-entropy loss\n",
    "        return sum(-np.log(ps[t][targets[t], 0]) for t in range(self.seq_length))\n",
    "    \n",
    "    def backward(self, xs, hs, ycap, targets): # ycap = prediction, targets = groundtruth\n",
    "        # compute the gradients going backwards\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0]) # the next stage of h\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ycap[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            \n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dc\n",
    "            \n",
    "            # dh has 2 compoentns, gradient flowing from output and from the next cell\n",
    "            dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "            \n",
    "            # dhrec is the recurring componenet seen in most of the calculation\n",
    "            dhrec = (1-hs[t] * hs[t]) * dh # backprop thru tanh non-linearity\n",
    "            db += dhrec\n",
    "            \n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(shrec, hs[t-1].T)\n",
    "            \n",
    "            # pass gradient from next cell for the next iteration\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        \n",
    "        # To mitigate gradient explosion, clip the gradients\n",
    "        \"\"\"\n",
    "        RNN can have problem about vanishing gradient or exploding gradient\n",
    "        meaning that the product of these gradients can goto 0 or increase exponentially\n",
    "        This makes it impossible for the model to learn\n",
    "        \"\"\"\n",
    "        for dparam in [dU,dW,dV,db,dc]:\n",
    "            np.clip(dparam, -5,5,out=dparam)\n",
    "        return dU,dW,dV,db,dc\n",
    "    \n",
    "    def update_model(self, dU,dW,dV,db,dc): # SGD\n",
    "        for param, dparam in zip([self.U,self.W, self.V, self.b, self.c], [dU,dW,dV,db,dc]):\n",
    "            # Change params according to gradients and learning rate\n",
    "            param += -self.lr*dparam\n",
    "    \n",
    "    def predict(self, data_reader, start, n):\n",
    "        # initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        \n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        \n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = zero_init(self.vocab_size, 1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-55ed3e3582cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "rnn.predict(data_reader, \"year\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
