"""
About: Build a model go from one sequence to another with pytorch and torchtext
    Model will be used to translate German to English translation.

Application: 
    Apply to problem involve going from one sequence to another
    + Summarization: Go from a sequence to a shorter sequence in the same language

Paper:
    https://arxiv.org/abs/1409.3215 

Methods: 
    DNN can't map sequences to sequences.
    Author use multilayeerd LSTM to marp the input sequence to a vector of fixd dimentionality
        and another deep LSTM to decode the target sequence from the vector.
"""

"""
Explanation of seq2seq moderls:
- The most common seq2seq models are encoder-decoder model with commonly use RNN  to encoder source/input
sentence into a single vector (context vector)
- The vector then being decoded by second RNN which learns to output the target/output sentence by generating it 1 word at a time
- Hidden states = vector representation of the sentence so far
- The hidden layer is either initialized to 0 or a learned param
- Once the final word, X, has been passed into RNN via embedding layer, we use the final hidden state as CONTEXT VECTOR. This vector represent the enture source sentence

Teacher Forcing = method for quickly and efficiently trainig RNN mdoel that use the ground truth from a prior time step as input
- this method uses ground truth as input instead of model output from the prior time step as input
- This is an alternative for BPTT - backprop thru time.
- Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1) rather than the output generated by the network
- When training,/testing out mode, we always know how many words are in our target senestence, so we stop generating words once we hit that many.
"""

"""
- Initialized model, input, output dm are defined by the size of the vocab
- The embedding dim and dropout for encoder and decoder can be difference but nuber of layer and size of hidden/cell state must be the same
"""

import torch
import torch.nn as nn
from utils.preprocess import *
from utils.nn_model import *
import time

#NEED: model, optimizer, loss function, training loop


# define the encoder, decoder, and seq2seq model
INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5

enc = Encoder(input_dim=INPUT_DIM, emb_dim=ENC_EMB_DIM, hid_dim=HID_DIM, n_layers=N_LAYERS, dropout=ENC_DROPOUT)
dec = Decoder(output_dim=OUTPUT_DIM, emb_dim=DEC_EMB_DIM, hid_dim=HID_DIM, n_layers=N_LAYERS, dropout=DEC_DROPOUT)
model = Seq2Seq(encoder=enc, decoder=dec, device=device).to(device)

# initialize the weight for the model: uniform distribution between -0.08 and +0.08
def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)

model.apply(init_weights) # apply called, then init_weights function will be called every module and sub-module within outmodel

# function calculate number of trainable param in the model
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad) # require gradient cal

print(f'The model has {count_parameters(model):,} trainable parameters')

optimizer = optim.Adam(model.parameters()) # optimizer

# loss function calculate both sofrmax as well as negative log-likelihood of the prediction

TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) # passing index of <pad> token as ignore we ignord the loss whenever thetarget token is a padding token

def train(model, iterator, optimizer, criterion, clip):

    """
    TRAINING: At each iteration:

        get the source and target sentences from the batch, $X$ and $Y$
        zero the gradients calculated from the last batch
        feed the source and target into the model to get the output, $\hat{Y}$
        as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view
            we slice off the first column of the output and target tensors as mentioned above
        calculate the gradients with loss.backward()
        clip the gradients to prevent them from exploding (a common issue in RNNs)
        update the parameters of our model by doing an optimizer step
        sum the loss value to a running total

    Finally, we return the loss that is averaged over all batches.
    """
    model.train()
    epoch_loss = 0
    for i, batch in enumerate(iterator): # trg is labels, src is input
        src = batch.src
        trg = batch.trg

        optimizer.zero_grad() 

        output = model(src, trg)

        #trg = [trg len, batch size]
        #output = [trg len, batch size, output dim]

        output_dim = output.shape[-1] #?

        output = output[1:].view(-1, output_dim) # prediction 
        trg = trg[1:].view(-1) # labels

        loss = criterion(output, trg) # calculates both the log softmax as well as the negative log-likelihood of our predictions. 

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # prevent explosion

        optimizer.step() # gradient descent / adam

        epoch_loss += loss.item() # sum all the loss
    return epoch_loss/len(iterator) # average the loss sum

def evaluate(model, iterator, criterion):
    """
    Similar to trainng loop but no need to pass an optimizer and clip value
    Set the model with model.eval
    Use torch.no_grad() to ensure no gradients are calculated , reduce memory consumptio and speed up things
    Iteration loop same but turn off teacher forcing  
    """
    model.eval()
    epoch_loss= 0
    with torch.no_grad():
        for i, batch in enumerate(iterable=iterator):
            src = batch.src
            trg = batch.trg
            output = model(src, trg, 0) # turn off teacher forcing

            #  trg = [trg len, batch size]
            #output = [trg len, batch size, output dim]
            output_dim = output.shape[-1]
            
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            #trg = [(trg len - 1) * batch size]
            #output = [(trg len - 1) * batch size, output dim]

            loss = criterion(output, trg)
            
            epoch_loss += loss.item()

    return epoch_loss/len(iterator) # average the loss sum

def epoch_time(start_time, end_time):
    """ Calculate epoch run time """
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

# check if our model has achieved best validation loss so far, if it has, we will update out best validation loss and save the parm to our model (State_dict)
# then test model with saved param

# Note that the model is super() so it use the same model
def main():

    N_EPOCHS = 10
    CLIP = 1
    best_valid_loss = float('inf')
    
    print("Running")
    for epoch in range(N_EPOCHS):
        start_time = time.time()
        train_loss = train(model=model, iterator=train_iterator, optimizer=optimizer, criterion=criterion, clip=CLIP)
        valid_loss = evaluate(model, valid_iterator, criterion=criterion)
        end_time = time.time()

        epoch_mins, epoch_secs = epoch_time(start_time=start_time, end_time=end_time)

        #save model for best validation set
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), 'seq2seq-model.pt')
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')


if __name__ == "__main__":
    main()