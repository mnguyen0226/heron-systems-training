"""
About: Build a model go from one sequence to another with pytorch and torchtext
    Model will be used to translate German to English translation.

Application: 
    Apply to problem involve going from one sequence to another
    + Summarization: Go from a sequence to a shorter sequence in the same language

Paper:
    https://arxiv.org/abs/1409.3215 

Methods: 
    DNN can't map sequences to sequences.
    Author use multilayeerd LSTM to marp the input sequence to a vector of fixd dimentionality
        and another deep LSTM to decode the target sequence from the vector.
"""

"""
Explanation of seq2seq moderls:
- The most common seq2seq models are encoder-decoder model with commonly use RNN  to encoder source/input
sentence into a single vector (context vector)
- The vector then being decoded by second RNN which learns to output the target/output sentence by generating it 1 word at a time
- Hidden states = vector representation of the sentence so far
- The hidden layer is either initialized to 0 or a learned param
- Once the final word, X, has been passed into RNN via embedding layer, we use the final hidden state as CONTEXT VECTOR. This vector represent the enture source sentence

Teacher Forcing = method for quickly and efficiently trainig RNN mdoel that use the ground truth from a prior time step as input
- this method uses ground truth as input instead of model output from the prior time step as input
- This is an alternative for BPTT - backprop thru time.
- Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1) rather than the output generated by the network
- When training,/testing out mode, we always know how many words are in our target senestence, so we stop generating words once we hit that many.

"""


def main():
    print("Running")

if __name__ == "__main__":
    main()