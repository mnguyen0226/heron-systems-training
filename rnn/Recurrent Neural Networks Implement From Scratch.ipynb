{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation from Scratch - Language Model\n",
    "- Understanding: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ \n",
    "- Implementation\n",
    "    - Implement a Language Model using RNN\n",
    "    - Why is this important? \n",
    "        - For scoring mechanism\n",
    "        - Because we can predict the probability of a word given the preceding words, we are able to generate next text. Meaning that given an existing sequence of words, we sample the next word from the predicted probabilities. We repeat this process till get the full sentence.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Applications:\n",
    "- Machine Translation systems\n",
    "- New Text Generation\n",
    "\n",
    "## 2. Understanding:\n",
    "- Hidden state acts as memory of the networks, it captures info about what happened in all previous time steps. The output at step o_t is calculated solely based on the memoery at time t. s_t can't capture info from too many time steps ago.\n",
    "- Unlike DNN, RNN shares the same param U,V,W across all steps meaning that we perform the same task at each step with just different inputs. This reduces the total number of param we need to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
